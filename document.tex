\documentclass[jou,a4paper,notxfonts]{apa}
\usepackage{graphicx} 
\usepackage{palatino}
\usepackage{fancyhdr}
\usepackage{url}
\pagestyle{fancy}


% header on first page
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyhead[L]{\small
Journal of Eye Movement Research\\
1(1):1, 1-2}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}



\title{Distributed Eye Tracking Network for Conveying Gaze of Remote Users in a Robotic Telepresence
Scenario}
%Fill this in


%please refer to http://www.ilsp.gr/homepages/protopapas/apacls.html#titlhead for different numbers of authors and affiliations
\threeauthors{First Author}{Second Author}{Third Author}
\threeaffiliations{First affiliation}{Second Affiliation}{Third Affiliation}


\journal{}
\volume{}

\abstract{Telepresence technology permits visiting a distant geographical location using a mobile robot. A panoramic
camera on the robot can provide remote users with an immersive viewing perspective of the robot's environment.
One limitation of telepresence is that for humans around the robot or elsewhere is difficult to gather a sense of where
in the robot's environment the remote users are paying attention to. This contrasts sharply with physical presence where
body language and other subtle cues provide a hint of gaze points on space. Here, we propose the usage of a distributed
network of eye trackers to monitor the gaze behavior and field of view within the panoramic image streamed by the robot
of the remote subjects. The robot then superimposes the remote user's gaze behavior into a condensed 2D representation
of the panoramic image it is currently capturing. We use as an illustrative application, a robot being used to visit a
museum remotely. A human museum guide directs the robot around the museum while interacting with the remote visitors.
The visualization of the remote visitors' gaze behavior provides a useful feedback signal to the museum tour guide about
the remote students areas of attention allowing online adaptations accordingly.
\linebreak \linebreak {\bf Keywords: eye tracking, gaze tracking, telepresence, eye tracking network, gaze aware
interfaces, pervasive computing, HCI, gaze responsive interfaces, human computer interaction}}

\acknowledgements{}
\shorttitle{}
\rightheader{}

% repeat the authors here (use et al if more than three authors):
\leftheader{ }

\begin{document}

\maketitle

\lhead{\small
Journal of Eye Movement Research\\
1(1):1, 1-2
}
\rhead{
Author, A., Author, B., & Author, C. (2007)\\
Short Title
}
\thispagestyle{plain}

\section{Introduction} 
Telepresence technology allows a person to feel and/or to have an effect as if they were present at a place other than
their real location. Telepresence technology requires that the remote user is provided with sensory stimuli conveying
information about the distant location in order to re-create the feeling of being in the distant location. This is
usually done via visual and audio sensors that capture and transmit the information about an environment to the remote
users. The option of interacting with the remote location through output effectors can be give to the remote users. This
can be achieved by sensing, transmitting and duplicating in the remote location the user's position, movements, actions,
or voice. Hence, in realistic telepresence sytems, information flows in both directions between the distant locations.
Telepresence systems range of possible usage scenarios have increased considerably in the last years thanks to advances
in robotics and telecommunication technologies. Among its many advantages telepresence systems enable collaboration
independent of geographic location and provide considerable savings in terms of time, travel cost and environmental
impact for numerous usage scenarios.


A robotic telepresence system is a particular subtype of telepresence technology that allows a subject to move virtually
through a distant location by remotely controlling a wheeled robot which is usually equipped with a camera, a microphone
and a loudspeaker. A screen in the mobile robot can optionally display live video of the subject’s face allowing the
remote user to interact with subjects around the robot. 


In this work, we augment an existing telepresence system that allows remote visitors to visit a museum using a semi
autonomous mobile robot, an immersive web interface and an educator or museum tour guide near the robot which guides the
tour. The system is designed to allow remote visitors to feel as they are on an actual tour in the museum  by employing
a web browser viewer of the panoramic video streamed by the robot. The innovative part of the system is the usage of a
network of eye trackers that monitor the gaze behavior of the remote museum visitors and transmits their gaze data to
the mobile robot. The robot then displays on its on-board screen the gaze behavior of the remote users in order to
provide to the educator at the museum with information about what is attracting the gaze (and hence the attention) of
the remote students.


The point of regard (PoR) of a computer user on the screen can be estimated using video oculography gaze tracking. The
usage of eye gaze as a pointing or control mechanism is well documented in the literature
\cite{duchowski, Rozado2012, Bonino2011, danhansenrevieweyetrackingmethods}. Gaze tracking has found a niche application
to study how computer users interact with content on a computer screen \cite{Rozado2012a, giaet, myiwann2011}. The gaze
data accumulated during a gaze tracking session can be visualized off-line as a heat map data structure. A heat map
captures the accumulated degree of attention that a given user places at different points on the screen during the gaze
tracking trial. Hence, it is well-established that the direction of gaze reflects the user's focus of attention on the
screen \cite{Zhai2003}, and hence, it provides a hint about intention. In this work, we leverage on these well known
facts about eye tracking practice and extend it to a network of eye trackers scenario used in combination to convey the
gaze behavior of several remote museum visitors in a remote telepresence systems. Since the gaze coordinates in a
remote eye tracker only reflect the PoR on a given plane, we also monitor the field of view within the panoramic video
viewer. The combination of the field of view and gaze coordinates maps to a unique point in 3D space that identifies the
position with the panoramic sphere video grabbed by the robot that is capturing the user's attention at any given point
in time.

% Literature review
The concept of telepresence systems is not new. The work from \cite{minsky1980telepresence} coined the term
\emph{telepresence} to describe systems that would transform work, manufacturing, energy production and medicine by
allowing remote workers to carry out tasks in remote locations through the usage of robots, sensors and tecommunication
infrastructure. As a pioneer in the field, Minsky was the first to proposed the idea of being able to work in
another country or planet through remote telepresence.

One of the first and most popular applications of telepresence is videoconferencing \cite{duncanson1973video}. The
review  from \cite{Egido:1988:VCT:62266.62268} highlighted some of the limitations of the technology at the time it was
first proposed, most of which have currently already been overcome \cite{lawson2010images}.

Another branch of telepresence that has gathered a lot of attention in the research literature is telesurgery, also
known as \emph{surgery at a distance} \cite{green1995telepresence}. The technique allows surgeons to operate
robotically on a patient that is located in different geographical coordinates. Among the many benefits of
the telesurgery concept, the technology has the potential to revolutionize healthcare delivery by bringing surgical
attention to previously inaccessible settings \cite{holt2004telesurgery}.


More recently, telepresence robots that are controlled remotely and allow a human to explore remote geographical
locations have been proposed \cite{schultz1991telepresence} and have received a considerable amount of research
attention \cite{tsui2011exploring}.

An important feature of robotic telepresence systems is their ability to realistically capture their surrounding
environment in order to convey a realistic representation of their location to the remote users. Panoramic images are a
key component of many modern robotic telepresence systems. Panoramic cameras provide the functionality of capturing and
transmitting a high quality visual representation of the robot's surrounding environment. The work from
\cite{Gledhill2003435} provides a good overview of panoramic imaging technologies summarizing and comparing some of the
methods used to carry out the different steps involved in the task: capturing, image processing, image stitching, 3-D
reconstruction, rendering and visualization.


Gaze tracking has been used extensively to monitor the gaze behavior of subjects in a variety of circumstances and
environments \cite{duchowski, jacob2003eye, myiwann2011}. However, to our knowledge, there is no literature available on
the utility of using gaze tracking to transmit the gaze behavior of remote users to the robot's environment in a
telepresence system. The amount of literature on using networks of eye trackers is also very limited due to the high
cost of the devices until very recently.


In Summary, in this work, we augment a robotic telepresence system that allows live interaction between remote students
and an educator in a museum by providing the musuem educator with a convinient visualization of the gaze dynamics of the
remote museum visitors. Multiple remote museum visitors connect via a single robot to the museum telepresence tour, but
each remote visitor controls their own view within the gallery. Remote students are given a tour of the museum by the
guide and can access additional digital content about objects on display through their panoramic viewers augmenting
thereby the tour experience. A network of eye trackers in the remote students' computers monitor and transmit their gaze
behavior. This gaze behavior data is displayed in the robot screen to provide the museum with a feedback signal about
where the remote visitors are paying attention to. This information closes the interaction loop between the educator in
the museum and the remote students and offsets some of the nonverbal communication limitations of telepresence systems
by signaling to the museum tour guide where the remote museum visitors are paying attention to at any given instant in
time.


%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
\section{Method}
The entire gaze monitoring and robotic telepresence system comprises several subparts that we described below.

\subsubsection{Eye-tracking system}
Eyes are used by humans to obtain information about the surroundings and to communicate information. When something
attracts our attention, we position our gaze on it, thus performing a \emph{fixation}. A fixation usually has a
duration of at least 150 milliseconds (ms). The fast eye movements that occur between fixations are known as
\emph{saccades}, and they are used to reposition the eye so that the object of interest is projected onto the fovea.
The direction of gaze thus reflects the focus of \emph{attention} and also provides an indirect hint for
\emph{intention}\cite{velichkovsky}.


A video-based gaze tracking system seeks to find where a person is looking, i.e. the Point of Regard (PoR), using images
obtained from the eye by one or more cameras. Most systems employ infrared illumination that is invisible to the human
eye and hence it is not distracting for the user. Infrared light improves image contrast and produces a reflection on
the cornea, known as corneal reflection or glint. Eye features such as the corneal reflections and the center of the
pupil/iris can be used to estimate the PoR. Figure \ref{screenGazeTracker} shows a screenshot of an eye being tracked by
the open-source ITU Gaze Tracker \cite{lowcostitugazetracker,Rozado2012}. In this case, the center of the pupil and two
corneal reflections are the features being tracked.


\begin{figure}[tp]
 %\fitbitmap[scale=0.5]{figures/screenGazeTracker.jpg}
 \includegraphics[width=0.45\textwidth, height=40mm]{figures/screenGazeTracker.jpg}
 \caption{\textbf{The Open Source ITU Gaze Tracker Tracking One Eye.} The
  features being tracked in the image are the pupil center and two corneal reflections. These features are used by the
  gaze estimation algorithms to determine the PoR of the user on the screen.}
 \label{screenGazeTracker}
\end{figure}

In most eye tracking studies, the PoR of the user is employed to generate a heat map of the scene observed or as a
pointing device that substitutes the mouse in gaze interaction paradigms. For this work, eye tracking was used behind
the scenes just to capture the gaze behavior of the remote museum visitors and transmit it to the robot in order for it
to display the aggregated gaze behavior of several users to the museum educator.

A set of 3 Tobii Rex eye trackers and 1 Tobii X1 light eye tracker are used for the experimental part. Both systems
track gaze at 30 frames per second (fps) with an average gaze estimation accuracy of 0.5 degrees at 60cm from the screen, 
\cite{TobiiTechnologyAB}. A simple exponential smoothing filter, \cite{Kumar2007}, of the raw gaze tracking data is
carried out to prevent jitter in the visualization of gaze.


\subsection{Robot}
% Fred, maybe you could provide more specific and technical details about this part?
We use a commercial robot that carries a head-height omnidirectional camera and a display screen, see
Figure \ref{MuseumRobot} to observe an early prototype of the system. The robot also carries a forward-looking camera
for video conferencing purposes between the museum tour guide and the remote students, two onboard computers and Wi-Fi
antennas.

\begin{figure}[tp]
 \begin{center}
 \includegraphics[width=0.25\textwidth, height=65mm]{figures/MuseumRobot.jpg}
 \end{center}
 %\fitbitmap{figures/ladybug.jpg}
 \caption{\textbf{Prototype of telepresence robot.} The figure displays the main components of the robot, a panoramic
 camera on its top, a screen in the middle and a mobile unit in the bottom.}
 \label{MuseumRobot}
\end{figure}

The robot moves at walking speed within any wheelchair accessible space. The robot is able to navigate
semi-autonomously to different locations in the museum under the supervision of the tour guide. Using light depiction
and ranging technology, the robot is able to detect walls, exhibits and people around and avoid them using a dynamic
obstacle avoidance system as part of its navigation system.  Hence, the robot can map its surroundings into a real-time
map of the gallery space that allows it to monitor its location in space.


\subsection{Museum Tour Guide}
The museum tour guide wears a wireless lapel microphone to ensure that he can be heard by the remote students while
carrying out a museum tour. The museum tour guide can see who is online and which students have questions via the
display on the front of the robot. With the proposed enhancement we are describing in this work, the museum guide is
also able to monitor the gaze of the remote visitors on the screen of the robot.


\subsection{360$^\circ$ Video and panoramic viewer}
A panorama is a single wide-angle image of the environment around the camera \cite{Gledhill2003435}. The most realistic
types surround the camera on the horizontal plane (360$^\circ$), and 180$^\circ$ in the vertical field of view. 
There are different ways to capture a panorama: single, rotated about its optical center, single omnidirectional camera
(using multiple cameras facing  in different directions) or using a stereo panoramic camera from which scene
information can be extracted.

The 360$^\circ$  panoramic camera (Figure \ref{ladybug}) used in our telepresence system was mounted on top of the robot
and captued a high-resolution omnidirectional image of the robot's environment. 

\begin{figure}[tp]
\begin{center}
 \includegraphics[width=0.25\textwidth, height=50mm]{figures/ladybug.jpg}
 \end{center}
 %\fitbitmap{figures/ladybug.jpg}
 \caption{The figure displays the ladybug 360$^\circ$ panoramic camera on top of the robot that uses several cameras
 facing different directions to create a panoramic image of the robots surroundings.}
 \label{ladybug}
\end{figure}


Several cameras capture multiple images of a scene from different viewpoints from which stereo information can be
calculated and then used to create a 3-D model of the scene in the form of a cubical or spherical panoramas, see Figures
\ref{panoramicImage} and \ref{omnidirectionalimage}.

\begin{figure}[tp]
 \begin{center}
 \includegraphics[width=0.45\textwidth, height=70mm]{figures/panoramicImage.jpg}
 \end{center}
 %\fitbitmap{figures/ladybug.jpg}
 \caption{\textbf{Creation of an Immersive Telepresence System.} The panoramic camera is used to capture several planes
 of the robot environment that are then stitched together in the panoramic viewer to create an immersive 3D-like
 experience for the remote users} 

 \label{panoramicImage}
\end{figure}

\begin{figure}[tp]
\begin{center}
 \includegraphics[width=0.45\textwidth, height=60mm]{figures/omnidirectionalimage.jpg}
 \end{center}
 %\fitbitmap{figures/ladybug.jpg}
 \caption{Images from the 360$^\circ$ camera on top of the robot are combined to form a high-resolution omnidirectional
 image that creates a 360$^\circ$ representation of the environment around the robot.}
 \label{omnidirectionalimage}
\end{figure}

The panoramic image viewer used in the remote computers was krpano\footnote{\url{http://krpano.com/}}. Krpano is a light
and very flexible high-performance viewer for all kind of panoramic images and interactive virtual tours. The viewer is
available as Flash and HTML5 application and is designed for the usage inside the Browser on Desktop and Mobiles
computer devices. Each remote students can then independently ``look around'' the gallery using the panoramic viewer
within their browser as shown in Figure \ref{museumtourview}.

\begin{figure}[tp]
 \fitbitmap{figures/museumtourview.jpg}
 \caption{Remote browser interface client through which student can look around the
museum with freedom to look at different areas within the 360 field of view environment}
 \label{museumtourview}
\end{figure}




\subsection{Remote Students' Client Computers}
Each student computer is equipped with a low cost eye tracker capturing gaze frames at 30Hz. The eye tracker provides
the X, Y coordinates of gaze on screen. Since the remote museum visitor can also control their horizontal and vertical
field of view within the panoramic camera, these parameters are also monitored by a javascript script running behind the
scenes in the browser. The horizontal and vertical view points refer to the views that determine the plane being
displayed on screen at any given time from the panoramic image. The combine parameters of horizontal, vertical field of
view and X, Y gaze coordinates are transformed in krpano into spherical coordinates to obtain the gaze point of regard
of the user in the spherical space. 


These four parameters define that define the point in 3D space the student is looking at are sent
back to the robot, see Figure \ref{aggregatedGazeBehavior}.

\begin{figure}[tp]
 \fitbitmap{figures/aggregatedGazeBehavior.jpg}
 \caption{Network of eye trackers on the remote students computer captures the gaze behavior of the students and
 combines the gaze data into a high-resolution omni-directional image}
 \label{aggregatedGazeBehavior}
\end{figure}

Images from the 360 degree camera on top of the robot are combined to form a condensed omni-directional image currently
being captured. The points of regard of several remote museum visitors are projected into this image to provide feedback
to the educator about where the students are paying attention to in the scene, see Figure \ref{aggregatedGazeBehavior}
and \ref{robotDisplay}.

\begin{figure}[tp]
 \includegraphics[width=0.4\textwidth, height=140mm]{figures/robotDisplay.jpg}
 \caption{Aggregated gaze behavior of remote
students superimposed into the omni-directional is shown in the robot display so the tour guide for instance can obtain
feedback about what areas in space the remote students are paying attention to}
 \label{robotDisplay}
\end{figure}


Each remote student computers uses a server-client arquitecture to gather the
data being monitored. A javascript client program embedded in the browser
displayed paga monitors the panoramic viewer field of view and stream this data
through a websocket to a local server that also receives gaze coordinates from a
gaze tracker client. This server streams the horizontal and vertical field of
view and the gaze coordinates to a further server located in the robot that
dispatches all the gaze data streams to a handler that displays this information
on top of a condensed representation of the omni-directional image currently
being captured by the robot.

The TCP protocol is used to issue commands to the eye trackers such as start
calibration, calculate results from calibration, start tracking, stop tracking.
Gaze estimation is sent through the UDP protocol, since the system can afford to
lose some packets since this would not impact the visualization of the gaze
behavior of remote museum visitors.


% ---------------------------------------------------------------
\section{Application}
The system described here is currently in the protopyting stage using a network
of 4 eye trackers. Our scripts in the browser client stream the field of view
parameters of each user and their gaze coordinates to a server, not yet
integrated into the robot, that parses and commands the display unit to render
the gaze coordinates of each user being monitored by representing his/her gaze
as a disc over a condensed 2D representation of the panoramic image. In order to
gather a sense about how the remote gaze monitoring system in the robotic
telepresence  scenario works, the interested reader is encouraged to take a look
at the manuscript's associated video at
\url{http://www.youtube.com/watch?v=gB89jT2_3oA}. The video provides a good
visual overview of the system at work and how it can be used to monitor the gaze
behavior of remote museum visitors. The video also shows how the gaze data of
the remote users is superimposed in the condensed 2D representation of the
panoramic image being captured by the robot.

The video shows how the museum tour guide moves around the museum closely follow
by the semi-autonomous robot. The remote students are able to customize the
specific view from the panoramic video stream that they want to take at any
particular time point.

At the present time, this project is still a work in progress that is currently
testing  the  performance  of  early prototypes of the system but that has
already  reached a proof of concept status. Future work will strive to carry out
user studies with young students (ages 10-16) from several national high schools
in order to study their gaze dynamics during a remote museum visits by
following their gaze patterns with a network of low cost Tobii Rex eye trackers. 



\section{Discussion}
Advancements in robotics and communication technologies have allowed the
emergence of telepresence technology systems that permit users to perceive
and/or interact with a distant region from a remote location.
Telepresence system offer many opportunities. Telepresence facilitates regions
of the world exporting specialized skills to anywhere. The technology offers
numerous advantages in terms of time and cost benefits by reducing
transportation costs and of energy and commuting time. Last but not least,
telepresence eliminates numerous chemical and physical health hazards of
physical presence in dangerous locations.

One particular are where telepresence systems can be particularly advantageous
is for education scenarios such as the system presented in this work that allows
potential museum visitors to visit an important institution with vasts amounts
of knowledge from anywhere. National institutions such as museums have a
responsibility to reach regional and remote potential visitors who are often
unable to visit them physically due to factors such as cost and distance. A
mobile telepresence system provides an opportunity for regional and remote
potential museum visitors to participate in tours on the museums and other
educationally relevant institutions.


A key challenge for robotic telepresence systems for visiting distant locations
through a robot will how to leverage the new opportunities provided by
telepresence technology to engage users of the system by providing highly
realistic user experiences. In the presented system the panoramic viewer in the
browser's client is highly immersive striving to make remote students feel as if
they are present in the gallery through the panoramic video stream and by being
highly interactive allowing remote students to engage in
discussions with the educator and other students on the tour. The strive
to make the tour a realistic experience does not preclude augmenting the
tour experience by superimposing in key objects of the museum meta
digital content layers for which they might wish to know more. 


The non-invasive nature of the eye trackers being uses, by virtue of being video
based and not requiring any equipment physically attached to the museum visitors
makes the system very comfortoble to use. The gaze tracking monitoring being
carried out in the background results transparent to the users that are not
constantly reminded that this task is being carried out. Rather the task is
carried out silently in the background.


The interactive nature of the systems presented here in the context of a museum
visit permits interactive learning rather than passive and collaborative
learning by allowing students to interact with each other and with the tour
guide. This visual aspect greatly enhances communications, allowing for
perceptions of facial expressions and other body language.


The importance of high quality sensory feedback is paramount in
telepresence scenarios. The network of eye trackers system presented
here provides an additional important clue to the museum tour guide about what
areas in his/her environment are attracting the student's attention.


One potential limitation of the system is the possibility of cognitively
overloading the museum tour guide that while carrying out his usual touring
tasks has to pay attention to the robot's display in order to monitor where in
the environment the students are paying attention to. This potential issue can
be limited by the tour guide consciously trying to avoid continuous monitoring
of the gaze behavior of remote visitors and concentrating on its traditional
tasks and looking at the display of the robot at key moments during the tour
where he feels he needs to obtain feedback about the engagement levels of the
tour participants.

These
eye trackers offer a resolution of 30 samples per second and produce gaze
estimation with an average accuracy of 0.5 degrees at 60 cm from the screen,
enough for the no high resolution intensive application described here.


The data gathered by the proposed system is simply used to provide a feedback
signal to the museum tour guide, but once the data has been gathered, all sort
of sophisticated studies about learning during the museum tour can be
envisioned. For instance, they gaze behavior of a class of students could be
monitor during the museum visit. After the visit, the students would be required
to answer a questionnaire  to measure the degree of understanding and acquired
knowledge  that they acquired through the visit. with these data in place,
correlations between the gaze behavior  of  students performing poorly  on the
questionnaire and that of the students performing well could generate 
interesting insights that could point out gaze dynamics specifics pointing
towards student becoming disengaged with the task. once these knowledge would be
in place, it could be online to recapture the attention of students that the
system detects that are starting to become distracted.


In summary, the system presented here is first of its kind in capturing the gaze
data of several museum visitors in order to teletransfer their gaze activity to
a museum tour guide. This important feedback signal helps to bridge the
communication gap between the museum tour educator and the distant museum
visitors. Further studies should build over the presented system and use the
gaze data to find out those students that are not benefiting from the museum
tour and provide them with online signals to recapture their attention in order
to improve their educational experience.


\bibliography{library}
\end{document}

